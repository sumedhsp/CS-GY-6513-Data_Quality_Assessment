{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0f10cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/20 18:42:43 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n",
      "23/11/20 18:42:43 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n",
      "23/11/20 18:42:43 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "23/11/20 18:42:43 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n"
     ]
    }
   ],
   "source": [
    "#  SPARK START!\n",
    "from pyspark import SparkContext, SparkConf\n",
    "cf = SparkConf()\n",
    "cf.set(\"spark.submit.deployMode\",\"client\")\n",
    "sc = SparkContext.getOrCreate(cf)\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "\t    .builder \\\n",
    "\t    .appName(\"dATA Quality data index\") \\\n",
    "\t    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "\t    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bda3a994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "da39a3ee5e6b4b0d3255bfef95601890afd80709\n",
      "da39a3ee5e\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from pyspark.sql.functions import col, when, count,isnan\n",
    "import hashlib\n",
    "import time\n",
    "\n",
    "hashlib.sha1().update(str(time.time()).encode(\"utf-8\"))\n",
    "print(hashlib.sha1().hexdigest())\n",
    "print(hashlib.sha1().hexdigest()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce108131",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a6f0b976",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataQualityIndexOf:\n",
    "    \n",
    "    def __init__(self, path_to_csv=None):\n",
    "        self.table_df = spark.read.csv(path=path_to_csv,header=True)\n",
    "        \n",
    "        hashlib.sha1().update(str(time.time()).encode(\"utf-8\"))\n",
    "        self.table_name = \"table_\"+hashlib.sha1().hexdigest()[:5]\n",
    "        \n",
    "        self.table_df.createOrReplaceTempView(self.table_name)\n",
    "        self.uniq = None\n",
    "        self.complete = None\n",
    "        self.complete_c = []\n",
    "        self.unique_c = []\n",
    "        \n",
    "    def load_from_csv(self, path_to_csv):\n",
    "        self.table_df = spark.read.csv(path=path_to_csv,header=True)\n",
    "        \n",
    "        hashlib.sha1().update(str(time.time()).encode(\"utf-8\"))\n",
    "        self.table_name = \"table_\"+hashlib.sha1().hexdigest()[:5]\n",
    "        \n",
    "        self.table_df.createOrReplaceTempView(self.table_name)\n",
    "        \n",
    "    def uniqueness(self):\n",
    "        total_rows = self.table_df.count()\n",
    "        \n",
    "        max_uniq= 0\n",
    "        \n",
    "        #constant number of columns so without losing much efficieny we can execute the below\n",
    "        #we can utilize the .map fnction but then we would have to convert the dataframe to a rdd first anyways which takes time\n",
    "        self.unique_c = []\n",
    "        for col in self.table_df.columns:\n",
    "            \n",
    "            uniq_i = self.table_df.select(col).distinct().count() / total_rows\n",
    "            # columns with the highest uniqueness\n",
    "            self.unique_c.append(uniq_i)\n",
    "            max_uniq = max(uniq_i,max_uniq) \n",
    "        #floor\n",
    "        \n",
    "        if max_uniq==1:\n",
    "            self.uniq=1\n",
    "            return max_uniq\n",
    "        self.uniq=0\n",
    "        return 0\n",
    "    \n",
    "    def completeness(self):\n",
    "        total_rows = self.table_df.count()\n",
    "        \n",
    "        total_complete = 0\n",
    "        complete_c = 0\n",
    "        \n",
    "        self.complete_c = []\n",
    "        for c in self.table_df.columns:\n",
    "            \n",
    "            df2 = parking_violation.table_df.filter( col(c) != \"Null\").filter(col(c) !='None').filter(col(c) != '' ).filter(col(c) != '-' ).filter(col(c).isNotNull() ) \n",
    "            \n",
    "            complete_c = df2.count() / total_rows\n",
    "            self.complete_c.append(complete_c)\n",
    "            \n",
    "            total_complete += complete_c\n",
    "        \n",
    "        \n",
    "        self.complete= total_complete / len(self.table_df.columns)\n",
    "        return self.complete\n",
    "    def getDQA(self):\n",
    "        uniq = self.uniqueness()\n",
    "        comp = self.completeness()\n",
    "        return uniq+comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "49cc7c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "parking_violation = DataQualityIndexOf('/shared/CS-GY-6513/parking-violations/parking-violations-header.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1ca8bd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.8949280660257894"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parking_violation.getDQA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "86785c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8949280660257893\n"
     ]
    }
   ],
   "source": [
    "print(parking_violation.complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5e14e1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(parking_violation.uniq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95597afb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
